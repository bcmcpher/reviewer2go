{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a195ce-7dc6-4e78-8015-448c59e807c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ee8678-a101-4f79-9cca-586da4077bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74da6f05-9316-4522-b969-77d070f220ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "\n",
    "TRUE = ['yes', 'true']\n",
    "FALSE = ['no', 'false']\n",
    "class Binary(BaseModel):\n",
    "    value: str = Field(description=\"yes or no value\", examples=['yes', 'no'])\n",
    "\n",
    "    def __bool__(self):\n",
    "        if self.value in TRUE:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "binary_parser = PydanticOutputParser(pydantic_object=Binary)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "template = \"You are a helpful assistant in the field of {field} that search if the concept of {concept} is present or not in the text.\\n {format_instructions}. \\n The text is :  \\n {text}\\n\"\n",
    "\n",
    "\n",
    "neuro_concept_present_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"text\", \"concept\"],\n",
    "    partial_variables={\"format_instructions\": binary_parser.get_format_instructions(), \"field\":\"neuroscience\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99a795ac-3eac-4eaf-832d-03810c36eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.output_parsers.openai_functions import (\n",
    "    JsonOutputFunctionsParser,\n",
    "    PydanticOutputFunctionsParser,\n",
    ")\n",
    "\n",
    "#function = _get_tagging_function(schema)\n",
    "#llm_kwargs = get_llm_kwargs(function)\n",
    "\n",
    "output_parser = JsonOutputFunctionsParser()\n",
    "\n",
    "chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=neuro_concept_present_prompt,\n",
    "        output_parser=output_parser,\n",
    "        #llm_kwargs=llm_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f8435b3-1fb6-426b-ac9f-5025bf6c99bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse function call: 'function_call'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\reviwer2go\\Lib\\site-packages\\langchain\\output_parsers\\openai_functions.py:73\u001b[0m, in \u001b[0;36mJsonOutputFunctionsParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m     function_call \u001b[38;5;241m=\u001b[39m \u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'function_call'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcept\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdespiking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI love despiking. It\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms help to understand the data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reviwer2go\\Lib\\site-packages\\langchain\\chains\\base.py:89\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m     85\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m     88\u001b[0m     config \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reviwer2go\\Lib\\site-packages\\langchain\\chains\\base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reviwer2go\\Lib\\site-packages\\langchain\\chains\\base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    301\u001b[0m     inputs,\n\u001b[0;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reviwer2go\\Lib\\site-packages\\langchain\\chains\\llm.py:104\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reviwer2go\\Lib\\site-packages\\langchain\\chains\\llm.py:261\u001b[0m, in \u001b[0;36mLLMChain.create_outputs\u001b[1;34m(self, llm_result)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         {\n\u001b[1;32m--> 261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    262\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[0;32m    263\u001b[0m         }\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[0;32m    265\u001b[0m     ]\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[0;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reviwer2go\\Lib\\site-packages\\langchain\\output_parsers\\openai_functions.py:78\u001b[0m, in \u001b[0;36mJsonOutputFunctionsParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse function call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m partial:\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Could not parse function call: 'function_call'"
     ]
    }
   ],
   "source": [
    "chain.invoke({\"concept\":\"despiking\", \"text\":\"I love despiking. It's help to understand the data\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6a8c8da-2170-4396-94c6-aab1ca58973f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JsonOutputFunctionsParser()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6061ee49-3b10-461c-8caa-001a9d25a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_concept_present = neuro_concept_present_prompt | llm | binary_parser\n",
    "small_chain = neuro_concept_present_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "324a8e2b-7df4-47a1-88d7-6e391eac5260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Binary(value='yes')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = is_concept_present.invoke({\"concept\":\"despiking\", \"text\":\"I love despiking. It's help to understand the data\"})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc0dae2b-cb7a-4e95-9f59-fc72fc5b59c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddac1c35-1b5c-4eb4-9954-b889f39d0e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant in the field of neuroscience that search if the concept of despiking is present or not in the text.\\n The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"value\": {\"title\": \"Value\", \"description\": \"yes or no value\", \"examples\": [\"yes\", \"no\"], \"type\": \"string\"}}, \"required\": [\"value\"]}\\n```. \\n The text is :  \\n I love life. It\\'s help to understand the data\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuro_concept_present_prompt.invoke({\"concept\":\"despiking\", \"text\":\"I love life. It's help to understand the data\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7684937c-3756-4535-8d3c-be0a027eddf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\"value\": \"yes\"}')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_chain.invoke({\"concept\":\"despiking\", \"text\":\"I love despiking. It's help to understand the data\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49b21e0e-ea73-48fc-96e4-8bcb25739e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ClusterMethod(BaseModel):\n",
    "    method: int = Field(description=\"number of cluster\", examples=[1,2,3])\n",
    "\n",
    "clustering_parser = PydanticOutputParser(pydantic_object=ClusterMethod)\n",
    "\n",
    "\n",
    "template = \"You are a helpful assistant in the field of {field} that tell how many {method} there is.\\n {format_instructions}. \\n The text is :  \\n {text}\\n\"\n",
    "\n",
    "\n",
    "clustering_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"text\", \"method\"],\n",
    "    partial_variables={\"format_instructions\": clustering_parser.get_format_instructions(), \"field\":\"neuroscience\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ea98929-1920-470b-9df7-869789a04c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_method_use = clustering_prompt | llm | clustering_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eefdb570-8735-494f-b3eb-b7ca72f8ec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClusterMethod(method=5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = cluster_method_use.invoke({\"method\":\"clustering\", \"text\":\"We used 5 cluster.\"})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e315656f-e0dd-4db0-aee1-ca37779d38f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cluster_method_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51449d2e-541d-4474-8953-e31291b5088f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81cdef03-fe0f-4723-8d6b-72312632de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-3.17.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading pypdf-3.17.2-py3-none-any.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.9 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/277.9 kB 435.7 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 30.7/277.9 kB 435.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  276.5/277.9 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.9/277.9 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-3.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21749bdb-6b49-4339-bbf4-3d304e90832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45dc084c-aec0-441c-a326-2cda1b489baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"2312.05250.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "153b7b9e-c100-47cb-974e-69b077d16c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='TaskMet: Task-Driven Metric Learning for Model Learning\\nDishank Bansal∗Ricky T. Q. Chen Mustafa Mukadam Brandon Amos\\nMeta\\nAbstract\\nDeep learning models are often deployed in downstream tasks that the training\\nprocedure may not be aware of. For example, models solely trained to achieve\\naccurate predictions may struggle to perform well on downstream tasks because\\nseemingly small prediction errors may incur drastic task errors. The standard end-\\nto-end learning approach is to make the task loss differentiable or to introduce a\\ndifferentiable surrogate that the model can be trained on. In these settings, the task\\nloss needs to be carefully balanced with the prediction loss because they may have\\nconflicting objectives. We propose take the task loss signal one level deeper than the\\nparameters of the model and use it to learn the parameters of the loss function the\\nmodel is trained on, which can be done by learning a metric in the prediction space.\\nThis approach does not alter the optimal prediction model itself, but rather changes\\nthe model learning to emphasize the information important for the downstream task.\\nThis enables us to achieve the best of both worlds: a prediction model trained in the\\noriginal prediction space while also being valuable for the desired downstream task.\\nWe validate our approach through experiments conducted in two main settings:\\n1) decision-focused model learning scenarios involving portfolio optimization\\nand budget allocation, and 2) reinforcement learning in noisy environments with\\ndistracting states. The source code to reproduce our experiments is available here.\\n1 Introduction\\ntrue modelMSETaskMetDFLmodel space\\ntask loss\\nFigure 1: The MSE results in a model\\nclose to the true model in the predic-\\ntion space, but may give poor task per-\\nformance. Decision-focused learning\\n(DFL) methods optimize the task loss,\\nbut may deviate from the prediction\\nspace. TaskMet optimizes the task loss\\nwhile retaining the prediction task.Machine learning models for prediction are typically trained\\nto maximize the likelihood on a training dataset. While the\\nmodels are capable of universally approximating the under-\\nlying data generating process to predict the output, they are\\nprone to approximation errors due to limited training data and\\nmodel capacity. These errors lead to suboptimal performance\\nin downstream tasks where the models are used. Furthermore,\\neven though a model may appear to have reasonable predictive\\nperformance on the metric and training data it was trained on,\\nsuch as the mean squared error, employing the model for a\\ndownstream task may require the model to focus on different\\nparts of the data that were not emphasized in the training for\\npredictive performance. Overcoming the discrepancy between\\nthe model’s prediction task and performance on a downstream\\ntask is the focus of our paper.\\n∗Work done as part of the Meta AI residency program.\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2312.05250v1  [cs.LG]  8 Dec 2023', metadata={'source': '2312.05250.pdf', 'page': 0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = loader.load_and_split()\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f144336-c5e2-4355-b5f5-1356886733e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'reviewer2go', 'Untitled.ipynb', 'Untitled1.ipynb']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c42d2220-c49d-4293-a61a-675ae279a654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table 1: Settings we focus on where there is a discrepancy between the prediction task of a model\\nand the downstream task where the model is deployed, i.e., Lpred̸=Ltask.\\n(x) ( y) ( Lpred) ( Ltask)\\nSetting Features Predictions Prediction Loss Task Loss\\nPortfolio optimization Stock information Expected return of a stock MSE Portfolio’s performance\\nBudget allocation Item information Value of item MSE Allocation’s performance\\nModel-based RL Current state and action Next state MSE Value estimation given the model\\nExamples of settings where the model’s prediction loss Lpredis mis-matched from the downstream\\ntaskLtaskinclude the following, which table 1 also summarizes:\\n1.theportfolio optimization setting from Wilder et al. [2019], which predicts the expected\\nreturns from stocks for a financial portfolio. Here, the Lpredis the MSE and Ltaskis from the\\nregret of running a portfolio optimization problem on the output;\\n2.theallocation setting from Wilder et al. [2019], which predicts the value of items that are\\nbeing allocated, e.g. click-through-rates for recommender systems. Here, Lpredis the MSE\\nandLtaskmeasures the result of allocating the highest-value items.\\n3.themodel-based reinforcement learning setting of learning the system dynamics from\\nNikishin et al. [2022]. Here, Lpredis the MSE of dynamics model and the Ltaskmeasures\\nhow well the agent performs for downstream value predictions.\\nMotivated by examples such as in table 1, the research topics of end-to-end task-based model learning\\n[Bengio, 1997, Donti et al., 2017], decision-focused learning [Wilder et al., 2019], and Smart “Predict,\\nthen Optimize” [Elmachtoub and Grigas, 2022] study how to use information from the downstream\\ntask to improve the model’s performance on that particular task. Task-based learning has applications\\nin financial price predictions [Bengio, 1997, Elmachtoub and Grigas, 2022], inventory stock, demand,\\nand price forecasting [Donti et al., 2017, Elmachtoub and Grigas, 2022, El Balghiti et al., 2019,\\nMandi et al., 2020, Liu et al., 2023], dynamics modeling for model-based reinforcement learning\\n[Farahmand et al., 2017, Amos et al., 2018, Farahmand, 2018, Bhardwaj et al., 2020, V oelcker et al.,\\n2022, Nikishin et al., 2022], renewable nowcasting [V ohra et al., 2023], vehicular routing [Shi and\\nTokekar, 2023], restless multi-armed bandits for maternal and child care [Wang et al., 2022], medical\\nresource allocation [Chung et al., 2022], and budget allocation, matching, and recommendation\\nproblems [Kang et al., 2019, Wilder et al., 2019, Shah et al., 2022].\\nLimitations of task-based learning. Task-based model learning comes with the goal of being able\\nto discover task-relevant features and data-samples on its own without the need of explicit inductive\\nbiases. The current trend for end-to-end model learning uses task loss along with the prediction loss\\nto train the prediction models. Though easy to use, these methods may be limited by 1) the prediction\\noverfitting to the particular task, rendering it unable to generalize; 2) the need to tuning the weight\\ncombining the task and prediction losses as in eq. (1).\\nOur contributions. We propose one way of overcoming these limitations: use the task-based\\nlearning signal not to directly optimize the weights of the model, but to shape a prediction loss\\nthat is constructed in a way so that the model will always stay in the original prediction space. We\\ndo this in section 3 via metric learning in the prediction space and use the task signal to learn a\\nparameterized Mahalanobis loss. This enables more interpretable learning of the model using the\\nmetric compared to learning with a combination of task loss and prediction loss. The learned metric\\ncan uncover underlying properties of the task that are useful for training the model, e.g. as in figs. 4\\nand 7. Section 4 shows the empirical success of metric learning on decision focused model learning'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "03a0b086-1cfe-4723-98cd-3edbc96dc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = ' '.join(page.page_content for page in pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2350ba1-699f-4422-a1d9-6ed86f311f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81120b1e-eec8-4cf7-8fa7-d91584beb38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TaskMet: Task-Driven Metric Learning for Model Learning\\nDishank Bansal∗Ricky T. Q. Chen Mustafa Mukadam Brandon Amos\\nMeta\\nAbstract\\nDeep learning models are often deployed in downstream tasks that the training\\nprocedure may not be aware of. For example, models solely trained to achieve\\naccurate predictions may struggle to perform well on downstream tasks because\\nseemingly small prediction errors may incur drastic task errors. The standard end-\\nto-end learning approach is to make the task loss differentiable or to introduce a\\ndifferentiable surrogate that the model can be trained on. In these settings, the task\\nloss needs to be carefully balanced with the prediction loss because they may have\\nconflicting objectives. We propose take the task loss signal one level deeper than the\\nparameters of the model and use it to learn the parameters of the loss function the\\nmodel is trained on, which can be done by learning a metric in the prediction space.\\nThis approach does not alter the optimal prediction model itself, but rather changes\\nthe model learning to emphasize the information important for the downstream task.\\nThis enables us to achieve the best of both worlds: a prediction model trained in the\\noriginal prediction space while also being valuable for the desired downstream task.\\nWe validate our approach through experiments conducted in two main settings:\\n1) decision-focused model learning scenarios involving portfolio optimization\\nand budget allocation, and 2) reinforcement learning in noisy environments with\\ndistracting states. The source code to reproduce our experiments is available here.\\n1 Introduction\\ntrue modelMSETaskMetDFLmodel space\\ntask loss\\nFigure 1: The MSE results in a model\\nclose to the true model in the predic-\\ntion space, but may give poor task per-\\nformance. Decision-focused learning\\n(DFL) methods optimize the task loss,\\nbut may deviate from the prediction\\nspace. TaskMet optimizes the task loss\\nwhile retaining the prediction task.Machine learning models for prediction are typically trained\\nto maximize the likelihood on a training dataset. While the\\nmodels are capable of universally approximating the under-\\nlying data generating process to predict the output, they are\\nprone to approximation errors due to limited training data and\\nmodel capacity. These errors lead to suboptimal performance\\nin downstream tasks where the models are used. Furthermore,\\neven though a model may appear to have reasonable predictive\\nperformance on the metric and training data it was trained on,\\nsuch as the mean squared error, employing the model for a\\ndownstream task may require the model to focus on different\\nparts of the data that were not emphasized in the training for\\npredictive performance. Overcoming the discrepancy between\\nthe model’s prediction task and performance on a downstream\\ntask is the focus of our paper.\\n∗Work done as part of the Meta AI residency program.\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2312.05250v1  [cs.LG]  8 Dec 2023 Table 1: Settings we focus on where there is a discrepancy between the prediction task of a model\\nand the downstream task where the model is deployed, i.e., Lpred̸=Ltask.\\n(x) ( y) ( Lpred) ( Ltask)\\nSetting Features Predictions Prediction Loss Task Loss\\nPortfolio optimization Stock information Expected return of a stock MSE Portfolio’s performance\\nBudget allocation Item information Value of item MSE Allocation’s performance\\nModel-based RL Current state and action Next state MSE Value estimation given the model\\nExamples of settings where the model’s prediction loss Lpredis mis-matched from the downstream\\ntaskLtaskinclude the following, which table 1 also summarizes:\\n1.theportfolio optimization setting from Wilder et al. [2019], which predicts the expected\\nreturns from stocks for a financial portfolio. Here, the Lpredis the MSE and Ltaskis from the\\nregret of running a portfolio optimization problem on the output;\\n2.theallocation setting from Wilder et al. [2019], which predicts the value of items that are\\nbeing allocated, e.g. click-through-rates for recommender systems. Here, Lpredis the MSE\\nandLtaskmeasures the result of allocating the highest-value items.\\n3.themodel-based reinforcement learning setting of learning the system dynamics from\\nNikishin et al. [2022]. Here, Lpredis the MSE of dynamics model and the Ltaskmeasures\\nhow well the agent performs for downstream value predictions.\\nMotivated by examples such as in table 1, the research topics of end-to-end task-based model learning\\n[Bengio, 1997, Donti et al., 2017], decision-focused learning [Wilder et al., 2019], and Smart “Predict,\\nthen Optimize” [Elmachtoub and Grigas, 2022] study how to use information from the downstream\\ntask to improve the model’s performance on that particular task. Task-based learning has applications\\nin financial price predictions [Bengio, 1997, Elmachtoub and Grigas, 2022], inventory stock, demand,\\nand price forecasting [Donti et al., 2017, Elmachtoub and Grigas, 2022, El Balghiti et al., 2019,\\nMandi et al., 2020, Liu et al., 2023], dynamics modeling for model-based reinforcement learning\\n[Farahmand et al., 2017, Amos et al., 2018, Farahmand, 2018, Bhardwaj et al., 2020, V oelcker et al.,\\n2022, Nikishin et al., 2022], renewable nowcasting [V ohra et al., 2023], vehicular routing [Shi and\\nTokekar, 2023], restless multi-armed bandits for maternal and child care [Wang et al., 2022], medical\\nresource allocation [Chung et al., 2022], and budget allocation, matching, and recommendation\\nproblems [Kang et al., 2019, Wilder et al., 2019, Shah et al., 2022].\\nLimitations of task-based learning. Task-based model learning comes with the goal of being able\\nto discover task-relevant features and data-samples on its own without the need of explicit inductive\\nbiases. The current trend for end-to-end model learning uses task loss along with the prediction loss\\nto train the prediction models. Though easy to use, these methods may be limited by 1) the prediction\\noverfitting to the particular task, rendering it unable to generalize; 2) the need to tuning the weight\\ncombining the task and prediction losses as in eq. (1).\\nOur contributions. We propose one way of overcoming these limitations: use the task-based\\nlearning signal not to directly optimize the weights of the model, but to shape a prediction loss\\nthat is constructed in a way so that the model will always stay in the original prediction space. We\\ndo this in section 3 via metric learning in the prediction space and use the task signal to learn a\\nparameterized Mahalanobis loss. This enables more interpretable learning of the model using the\\nmetric compared to learning with a combination of task loss and prediction loss. The learned metric\\ncan uncover underlying properties of the task that are useful for training the model, e.g. as in figs. 4\\nand 7. Section 4 shows the empirical success of metric learning on decision focused model learning and 7. Section 4 shows the empirical success of metric learning on decision focused model learning\\nand model-based reinforcement learning. Figure 1 illustrates the differences to prior methods.\\n2 Background and related work\\nTask-based model learning . We will mostly focus on solving regression problems where the dataset\\nD:={(xi, yi)}N\\ni=1consists of Ninput-output pairs, which we will assume to be in Euclidean space.\\nThe model makes a prediction ˆy:=fθ(x)and is parameterized by θ. The model has an associated\\nprediction loss, Lpred, and is used in conjunction with some downstream task that provides a task loss,\\nLtask, which characterizes how well the model performs on the task. The most relevant related work\\n2 to ours includes the approaches of Bengio [1997], Donti et al. [2017], Farahmand et al. [2017], Kang\\net al. [2019], Wilder et al. [2019], Nikishin et al. [2022], Shah et al. [2022], V oelcker et al. [2022],\\nNikishin et al. [2022], Anonymous [2023], Shah et al. [2023], which learn the optimal prediction\\nmodel parameter θto minimize the task loss Ltask:\\nθ⋆:= arg min\\nθLtask(θ) +αLpred(θ), (1)\\nwhere αis a regularization parameter to weigh the prediction loss which is MSE error (eq. (2))\\nin general. Alternatives to eq. (1) include 1) Smart, “Predict, then Optimize” (SPO) methods\\n[Elmachtoub and Grigas, 2022, El Balghiti et al., 2019, Mandi et al., 2020, Liu et al., 2023], which\\nconsider surrogates for when the derivative is undefined or uninformative, or 2) changing the\\nprediction space from the original domain into a latent domain with task information, e.g. task-\\nspecific latent dynamics for RL [Hafner et al., 2019b,a, Hansen et al., 2022]. Extensions such as\\nGupta and Zhang [2023], Zharmagambetov et al. [2023], Ferber et al. [2023] learn surrogates to\\novercome computationally expensive losses in eq. (1). Sadana et al. [2023] provide a further survey\\nof this research area.\\nSeparate from above line of work, the computer vision and NLP communities have also considered\\ntask-based losses for models: [Pinto et al., 2023] tune vision models with task rewards, e.g. for\\ndetection, segmentation, colorization, and captioning; Wu et al. [2021] consider representation\\nlearning for multiple tasks, Fernando and Tsokos [2021], Phan and Yamamoto [2020] consider\\nweighted loss for class imbalance problems in classification, object detection.\\nWorks such as Farahmand et al. [2017], V oelcker et al. [2022] use task loss in a different way\\ncompared to the above methods. They use task loss as a weighting term in the MSE loss itself. So\\nthe models are trained to focus more on samples with higher task loss. In their work, the task is the\\nestimation of the value function in model-based RL. This can be seen as the instantiation of our work\\nwhere the task loss is directly used as a metric instead of learning a metric.\\nOther related work on metric learning such as Hastie and Tibshirani [1995], Yang and Jin [2006],\\nWeinberger and Tesauro [2007], Kulis et al. [2013], Hauberg et al. [2012], Kaya and Bilge [2019]\\noften learns a non-Euclidean metric or distance that captures the geometry of the data and then solves\\na prediction task such as regression, clustering, or classification in that geometry. Other methods such\\nas V oelcker et al. [2022] can handcraft metrics based on domain knowledge. In contrast to these, in\\nthe task-based model learning, we propose that the downstream task (instead of the data alone) gives\\nthe relevant metric for the prediction, and that it is possible to use end-to-end learning as in eq. (4) to\\nobtain the task-based metric.\\nHow our contribution fits in. The mentioned methods mainly deal with using task-based losses to\\ncondition the model learning either by differentiation through task loss to update the model or using\\nit directly as weighing for MSE prediction loss. Whereas our work focuses on using task loss to learn\\na parameterized prediction loss which is then used to train the model. The task loss is not directly\\nused for model training.\\n3 Task-driven metric learning for model learning\\nWe first present why it’s useful to see the prediction space as a non-Euclidean metric space with an\\nunknown metric, then show how task-based learning methods can be used to learn that metric.\\n3.1 Metrics in the prediction space — Mahalanobis losses\\nWhen defining a loss on the model, we are forced to make a choice about the geometry to quantify\\nhow good a prediction is. This geometric information is often implicitly set in standard learning\\nsettings and there are often no other reasonable choices without more information. For example, a\\nsupervised model fθparameterized by θis often trained with the mean squared error (MSE)\\nθ⋆\\nMSE:= arg min\\nθE(x,y)∼D\\x02 supervised model fθparameterized by θis often trained with the mean squared error (MSE)\\nθ⋆\\nMSE:= arg min\\nθE(x,y)∼D\\x02\\n(fθ(x)−y)2\\x03\\n. (2)\\nThe MSE makes the assumption that the geometry of the prediction space is Euclidean. While it is\\na natural choice, it may not be ideal when the model needs to focus on important parts of the data\\nthat are under-emphasized under the Euclidean metric. This could come up by needing to emphasize\\nsome samples over others, or some dimensions of the prediction space over others.\\n3 y0y1y⋆\\ny0y1y⋆\\ny0y1y⋆Λϕ(x)\\x14\\n1 0\\n0 1\\x15 \\x14\\n2 0\\n0 1\\x15 \\x14\\n10 0\\n0 1\\x15\\nFigure 2: Examples of the Mahalanobis loss from eq. (3) in a 2-dimensional prediction task. The\\nmodel’s loss is zero only when ˆy=y⋆. Here, the metric Λϕ(x)increases the weighting on the y0\\ncomponent of the loss and thus emphasizes the predictions along this dimension.\\nWhile there are many possible geometries and metric spaces that could be defined over prediction\\nspaces, they are difficult to specify without more information. We focus on the metric space defined\\nby the Mahalanobis norm ∥z∥M:=\\x00\\nz⊤Mz\\x011/2, where Mis a positive semi-definite matrix. The\\nMahalanobis norm results in the prediction loss\\nLpred(θ, ϕ):=E(x,y)∼Dh\\n∥fθ(x)−y∥2\\nΛϕ(x)i\\n, (3)\\nwhere Λϕis a metric parameterized by ϕand this is also conditional on the feature xso it can learn\\nthe importance of the regression space from each part of the feature space.\\nMany methods can be seen as hand-crafted ways of setting a Mahalanobis metric, including: 1)\\nnormalizing the input data by making the metric appropriately scale the dimensions of the prediction,\\n2) re-weighting the samples as in Donti et al. [2017], Lambert et al. [2020] by making the metric\\nscale each sample based on some importance factor, or 3) using other performance measures, such as\\nthe value gradient in V oelcker et al. [2022].\\nMore generally beyond these, the Mahalanobis metrics help emphasize the:\\n1.relative importance of dimensions . the metric allows for down- or up-weighting different\\ndimensions of the prediction space by changing the diagonal entries of the metric. Figure 2\\nillustrates this.\\n2.correlations in the prediction space . the quadratic nature of the loss with the metric allows\\nthe model to be aware of correlations between dimensions in the prediction space.\\n3.relative importance of samples . heteroscedastic metrics Λ(x)enable different samples to be\\nweighted differently for the final expected cost over the dataset.\\nWithout more information, parameterizing and specifying the best metric for learning the model\\nis challenging as it involves the subproblem of understanding the relative importance between\\npredictions. We suggest that when it is available, the downstream task information characterizing the\\noverall model’s performance can be used to learn a metric in the prediction space. Hence, learning\\nmodel parameters with a metricized loss can be seen as conditioning the learning problem. The ability\\nto learn the metric end-to-end enables the task to condition the learning of the model in any or all of\\nthe three ways described above. This approach offers an interpretable method for the task to guide the\\nmodel learning, in contrast to relying solely on task gradients for learning model parameters, which\\nmay or may not align effectively with the given prediction task.\\n4 Λϕ θ⋆(ϕ) Ltaskarg min\\nθLpred(θ, ϕ) ˆy=fθ⋆(ϕ)(x)\\n∂θ⋆(ϕ)\\n∂ϕvia IFT ∇θLtaskx: features\\ny: targets\\nΛϕ: metric\\nLpred(θ, ϕ):=Eh\\n∥fθ(x)−y∥2\\nΛϕ(x)i\\nFigure 3: TaskMet learns a metric for predictions with the gradient from a downstream task loss.\\n3.2 End-to-end metric learning for model learning\\nOur key idea is to learn a metric in the form of eq. (3) end-to-end with a given task, which is then used\\nto train the prediction model. Figure 3 and alg. 1 summarize this approach. The learning problem of\\nthe metric and model parameters are formulated as the bilevel optimization problem\\nϕ⋆:= arg min\\nϕLtask(θ⋆(ϕ)), (4)\\nsubject to θ⋆(ϕ) = arg min\\nθLpred(θ, ϕ) (5)\\nwhere ϕandθare (respectively) the metric and model parameters, Lpredis the metricized prediction\\nloss (eq. (3)) to train the prediction model, and Ltaskis the task loss defined by the task at hand (which\\ncould be another optimization problem, e.g. eq. (8), or another learning task, e.g. eq. (10).\\nGradient-based learning. We learn the optimal metric Λϕ⋆with the gradient of the task loss, i.e.\\n∇ϕLtask(θ⋆(ϕ)). Using the chain rule and assuming we have the optimal θ⋆(ϕ)for some metric\\nparameterization ϕ, this derivative is\\n∇ϕLtask(θ⋆(ϕ)) =∇θLtask(θ)\\x0c\\x0c\\nθ=θ⋆(ϕ)·∂θ⋆(ϕ)\\n∂ϕ(6)\\nTo calculate the term ∇ϕLtask(θ⋆(ϕ)), we need to compute two gradient terms: ∇θLtask(θ)\\x0c\\x0c\\nθ=θ⋆(ϕ)\\nand∂θ⋆(ϕ)/∂ϕ. The former can be estimated in standard way since Ltask(θ)is an explicit function\\nofθ. However, the latter cannot be directly calculated because θ⋆is a function of optimization\\nproblem which is multiple iterations of gradient descent, as shown in eq. (5). Backpropping through\\nmultiple iterations of gradient descent can be computationally expensive, so we use the implicit\\nfunction theorem (appendix A) on the first-order optimality condition of eq. (5), i.e.∂Lpred(θ,ϕ)\\n∂θ= 0.\\nCombining these, ∇ϕLtask(θ⋆(ϕ))can be computed with\\n∇ϕLtask(θ⋆(ϕ)) =∇θLtask(θ)· −\\x12∂2Lpred(θ, ϕ)\\n∂θ2\\x13−1∂2Lpred(θ, ϕ)\\n∂ϕ∂θ\\x0c\\x0c\\x0c\\x0c\\x0c\\nθ=θ⋆(ϕ)| {z }\\n∂θ⋆/∂ϕ(7)\\nThe implicit derivatives in eq. (7) may be challenging to compute or store in memory because the\\nHessian term ∂2Lpred(θ, ϕ)/∂θ2is the Hessian of the prediction loss with respect to the model’s\\nparameters. Approaches such as Lorraine et al. [2020] are able to scale related implicit differentiation\\nproblems to models with millions of hyper-parameters. The main insight is that the Hessian does\\nnot need to be explicitly formed or inverted and the entire implicit derivative term needed for\\nbackpropagation can be obtained with an implicit solver. We follow Blondel et al. [2022] and\\ncompute the implicit derivative by using conjugate gradient on the normal equations.\\n5 Algorithm 1 TaskMet: Task-Driven Metric Learning for Model Learning\\nModels: predictor fθand metric Λϕwith initial parameterizations θandϕ\\nwhile unconverged do\\n// approximate θ⋆(ϕ)given the current metric Λϕ\\nforiin1. . . K do\\nθ←update (θ,∇θLpred(θ, ϕ))// fit the predictor fθto the current metric loss (eq. (3))\\nend for\\nϕ←update (ϕ,∇ϕLtask)// update the metric Λϕwith the task loss (eq. (6))\\nend while\\nreturn optimal predictor fθand metric Λϕsolving the bi-level problem in eq. (4)\\n4 Experiments\\nWe evaluate our method in two distinct settings: 1) when the downstream task involves an optimization\\nproblem parameterized by the prediction model output, and 2) when the downstream task is another\\nlearning task. For the first setting, we establish our baselines by replicating experiments from\\nprevious works such as Shah et al. [2022] and Wilder et al. [2019]. These baselines encompass tasks\\nlike portfolio optimization and budget allocation. In the second setting, we focus on model-based\\nreinforcement learning. Specifically, we concentrate on learning a dynamics model (prediction\\nmodel) and aim to optimize the Q-value network using the learned dynamics model for the Cartpole\\ntask [Nikishin et al., 2022]. Appendix B provides further experimental details and hyper-parameter\\ninformation.\\n4.1 Metric parameterization\\nWe parameterize the metric using a neural network with parameters ϕ, denoted as Λϕ:=L⊤\\nϕLϕ,\\nwhere Lϕis an n×nmatrix, where nis the dimension of the prediction space. This particular\\nfactorization constraint ensures that the matrix is positive semi-definite, which is crucial for it to be\\nconsidered a valid metric. The neural network parameters are initialized to make Λϕcloser to the\\nidentity matrix I, representing the Euclidean metric. The learned metric can be conditional on the\\ninput, denoted as Λϕ(x), or unconditional, represented as Λϕ, depending on the problem’s structure.\\n4.2 Decision-Focused Learning\\n4.2.1 Background and experimental setup\\nWe use three standard resource allocation tasks for comparing task-based learning methods [Shah\\net al., 2022, Wilder et al., 2019, Donti et al., 2017, Futoma et al., 2020]. In this setting, resource utility\\nprediction based on some input features constitute a prediction model, resource allocation constitutes\\nthe downstream task which is characterized by LtaskThe prediction model’s output parameterized the\\ndownstream resource optimization. The settings are implemented exactly as in Shah et al. [2022] and\\nhave task losses defined by\\nLtask(θ):=E(x,y)∼D[g(z⋆(ˆy), y)] (8)\\nwhere z⋆(ˆy):= arg minzg(z,ˆy)andg(z, y′)is some combinatorial optimization objective over\\nvariable zparameterized by y′. The task loss Ltaskis the expected value of objective function with\\ndecision variable z⋆(ˆy)induced by the prediction model ˆy=fθ(x)under the ground truth parameters\\ny. We use corresponding surrogate losses to replicate the z⋆(ˆy)optimization problem as in Shah\\net al. [2022], Wilder et al. [2019], Xie et al. [2020] and differentiate through the surrogate using\\ncvxpylayers [Agrawal et al., 2019].\\nThese settings evaluate the ability of TaskMet to capture the correlation between model predictions\\nand differentiate between different data-points in accordance to their importance for the optimization\\nproblem. Hence, we consider a heteroscedastic metric, i.e., Λϕ(x).\\n6 Table 2: Normalized test decision quality (DQ)\\non the decision oriented learning problems.\\nProblems\\nMethod αCubic Budget Portfolio\\nMSE −0.96±0.02 0.54±0.17 0.33±0.03\\nDFL 0 0.61±0.74 0.91±0.06 0.25±0.02\\nDFL 10 0 .62±0.74 0.81±0.11 0.34±0.03\\nLODL 00.96±0.005 0.84±0.105 0.17±0.05\\nLODL 10−0.95±0.005 0.58±0.14 0.30±0.03\\nTaskMet 0.96±0.005 0.83±0.12 0.33±0.03\\n0=random model 1=oracle modelTable 3: Test prediction errors (MSE) on the deci-\\nsion oriented learning problems.\\nProblems\\nMethod α Cubic Budget (×1e−4)Portfolio (×1e−4)\\nMSE 2.30±0.03 4.32±2.35 4.03±0.24\\nDFL 0 2.89±0.32 71.7±58.3 8.0e3±8e2\\nDFL 10 2.41±0.05 8.09±12.1 5.18±0.46\\nLODL 0 2.88±0.03 35.9±12.9 55.6±9.95\\nLODL 10 2.29±0.19 5.05±1.88 4.31±0.31\\nTaskMet 2.89±0.03 9.74±13.79 4.69±0.56\\nαis the prediction loss weight in eq. (1)\\nBaselines. We compare with standard baseline losses for learning models:\\n1.The standard MSE loss θ⋆= arg minθE(x,y)∼D[(fθ(x)−y)2]. This method doesn’t use\\nany task information.\\n2.DFL [Wilder et al., 2019], which trains the prediction model with a weighted combination\\nofLtaskandLpredas in eq. (1).\\n3.LODL Shah et al. [2022], which learns a parametric loss for each point in the training\\ndata to approximate the Ltaskaround that point. That is, LODL ψn(ˆyn)≈ L task(ˆyn)for\\nalln. They create a dataset of {(ˆyn,Ltask(ˆyn))}forˆynsampled around the yn. After this\\nthey learn the LODL loss for each point as ψ⋆\\nn= arg minψn1\\nKPK\\nk=1(LODL ψn(yk\\nn)−\\nLtask(yk\\nn))2. We chose the “Quadratic” variant of their method which is the closest to\\nours, where LODL ψn(ˆy) = (ˆy−y)⊤ψn(ˆy−y)where ψnis a learned symmetric Positive\\nsemidefinite (PSD) matrix. LODL also uses eq. (1) to learn the model parameters, but using\\nLODL ψn(ˆyn)≈ L task(ˆyn)\\nExperimental settings. We use the following experimental settings from [Shah et al., 2022]:\\n1.Cubic : This setting evaluates methods under model mismatch scenario where the model\\nbeing learned suffers with severe approximation error. In this task, it is important for\\nmethods to allocate model capacity to the points more critical for the downstream tasks.\\nPrediction Model : A linear prediction model fθ(x):=θxis learned for the problem where\\nthe ground truth data is generated by cubic function, i.e., yi= 10x3\\ni−6.5xi, xi∈U[−1,1].\\nDownstream task : Choose top B= 1 out of M= 50 resources ˆy= [ˆy1, . . . , ˆyM],\\nz⋆(ˆy):= arg maxiˆy\\n2.Budget Allocation : Choose top B= 2websites to advertise based on Click-through-rates\\n(CTRs) predictions of Kusers on Mwebsites.\\nPrediction Model :ˆym=fθ(xm)where xmis given features of mthwebsite and ˆym=\\n[ˆym,1, . . . , ˆym,K]is the predicted CTRs for mthwebsite for all Kusers.\\nDownstream task : Determine B= 2websites such that the expected number of users that\\nclick on the ad at least once is maximized, i.e., z⋆(ˆym) = arg maxzPK\\nj=0(1−QM\\ni=0zi·ˆyij)\\nwhere zi∈ {0,1}.\\n3.Portfolio Optimization : The task is to choose a distribution over Mstocks in Markowitz\\nportfolio optimization [Markowitz and Todd, 2000, Michaud, 1989] that maximizes the\\nexpected return under the risk penalty.\\nPrediction Model : Given the historical data xmabout a stock m, predict the future stock\\nprice ˆym. Combining prediction over Mstocks to get ˆy= [ˆy1, . . . , ˆyM].\\nDownstream Task : Given the correlation matrix Qof the stocks, choose a distribution over\\nstocks z⋆(ˆy) = arg maxzz⊤ˆy−λz⊤Qzs.t.PM\\ni=0zi≤1and 0≤zi≤1,∀i\\nWe run our own experiments for LODL [Shah et al., 2022] using their public code.\\n7 ModelQ*LossMetricPlanningActingLearningIFTIFTBackpropOMDTaskMetFigure 5: OMD [Nikishin et al., 2022] uses the planning task loss to learn the model parameters using\\nimplicit gradients. TaskMet add one more optimization step over OMD and instead of learning the\\nmodel parameters using task loss, we learn the metric which then is used to learn model parameters.\\n4.2.2 Experimental results\\nTaskMet MSE\\n−303\\ny(x)\\n−1 0 1\\nx012\\nΛ(x)\\nFigure 4: (Cubic problem) TaskMet learns\\na metric that prioritizes points that are the\\nmost important the downstream task. The\\neuclidean metric (MSE) puts equal weight\\non all points and leads to a bad model with\\nrespect to the downstream task.Table 2 presents a summary of the performance of differ-\\nent methods on all the tasks. Each problem poses unique\\nchallenges for the methods. The cubic setting suffers\\nfrom severe approximation errors, hence the learning\\nmethod needs to allocate limited model capacity more\\ntowards higher utility points compared to lower utility\\npoints. The MSE method performs the worst as it lacks\\ntask information and only care about prediction error.\\nDFL with α= 0performs better than MSE, but it can\\nget trapped in local optima, leading to higher variance\\nin the problem [Shah et al., 2022]. LODL ( α= 0) per-\\nforms among the highest in this problem since it uses\\nlearned loss for each point. TaskMet performs as good\\nas LODL as it can capture the relative importance of\\nhigher utility points versus lower utility points using the\\nlearned metric, resulting in more accurate predictions for\\nthose points (see fig. 4). In budget allocation , DFL (with\\nα= 0) performs the best, since it is solely optimizer\\noverLtask, but on the other hand it has 10orders of larger\\nprediction error as shown in table 3 indicating that the\\nmodel is overfit to the task, LODL ( α= 0) suffers from\\nthe same problem. TaskMet has the 2ndbest Decision\\nQuality without overfitting on the task, i.e., low prediction error. In Portfolio Optimization , the\\ndecision quality correlates highly with the model accuracy/prediction error as in this setting the\\noptimization problem mostly depends upon the accurate prediction of the stocks. This is the reason\\nthat MSE, DFL ( α= 10 ) performs the best, but DFL ( α= 0) performs the worst, since it has solely\\nbeing trained on Ltaskwithout any Lpred. As shown in table 2 and table 3, TaskMet is the only method\\nthat consistently performs well considering both task loss and prediction loss, across all the problem\\nsettings, this is due to the ability of the metric to infer problem-specific features without manual\\ntuning, unlike other methods.\\n4.3 Model Based Reinforcement Learning\\n4.3.1 Background and experimental setup\\nModel-based RL suffers from objective-mismatch [Bansal et al., 2017, Lambert et al., 2020]. This is\\nbecause dynamics models trained for data likelihood maximization do not translate to optimal policy.\\nTo reduce objective-mismatch, different losses [Farahmand et al., 2017, V oelcker et al., 2022] have\\nbeen proposed to learn the model which is better suited to learning optimal policies. TaskMet provides\\nan alternative approach towards reducing objective-mismatch, as the prediction loss is directly learnt\\nusing task loss. We set up the MBRL problem as follows. Given the current state stand control atat\\na timestep tof a discrete-time MDP, the dynamics model predicts the next state transition, i.e. ˆst+1:=\\nfθ(st, at). The prediction loss is commonly the squared error loss Est,at,st+1∥st+1−fθ(st, at)∥2\\n2,\\nand the downstream task is to find the optimal Q-value/policy. Nikishin et al. [2022] introduces idea\\nofoptimal model design (OMD) to learn the dynamics model end-to-end with the policy objective\\n8 16 32 64 128 256 512\\n# Distractors100200300400500Episode Return\\nTaskMet MLE OMD\\nFigure 6: Results on the cartpole with\\ndistracting states [Nikishin et al., 2022].\\n123456789101112...state dimension\\n16\\n32\\n64\\n128\\n256\\n512# distractorsmetric values (darker=higher)real states\\nFigure 7: Our learned metric successfully distinguishes\\nthe real states from the distracting states, i.e. the real\\nstates take a higher metric value.\\nvia implicit differentiation. Let Qω(s, a)be the action-conditional value function parameterized by\\nω. The Q network is trained to minimize the Bellman error induced by the model fθ:\\nLQ(ω, θ):=Es,a[Qw(s, a)−BθQ¯w(s, a)]2, (9)\\nwhere ¯ωis moving average of ωandBθis the model-induced Bellman operator BθQ¯w(s, a):=\\nrθ(s, a)+γEpθ(s,a,s′)[logP\\na′expQ(s′, a′)]. Q-network optimality defines ωas an implicit function\\nof the model parameters θasω⋆(θ) = arg minωLQ(ω, θ) =⇒∂LQ(ω,θ)\\n∂ω= 0. Now we have task\\nloss which is optimized to find optimal Q-values:\\nLtask(ω⋆(θ)):=Es,a[Qω⋆(θ)(s, a)−BQ¯ω(s, a)]2(10)\\nwhere the Bellman operator induced by ground-truth trajectory and reward is BQ¯ω(s, a):=r(s, a) +\\nγEs,a,s′logP\\na′expQ¯ω(s′, a′).\\nOMD setup. OMD end-to-end optimizes the model for the task loss, i.e. θ⋆= arg minθLtask(ω⋆(θ)).\\nTaskMet setup. For metric learning, we extend OMD to learn a metric using task gradients, to train\\nthe model parameters, see fig. 5. Metric learning just adds one more level of optimization to OMD\\nand results in the tri-level problem\\nϕ⋆= arg min\\nϕLtask(ω⋆)\\nsubject to ω⋆(θ⋆) = arg min\\nωLQ(ω, θ⋆)\\nθ⋆(ϕ) = arg min\\nθLpred(ϕ, θ)(11)\\nwhere Ltask(ω⋆)andLQ(ω, θ⋆)are defined in eq. (10) and eq. (9), respectively, and Lpred(ϕ, θ) =\\nEst,at,st+1∥st+1−fθ(st, at)∥2\\nΛϕ(st)is the metricized prediction loss in eq. (3).\\nTo learn ϕ⋆using gradient descent, we estimate ∇ϕLtaskas\\n∇ϕLtask=∇ωLtask(ω⋆)·∂ω⋆\\n∂θ⋆·∂θ⋆\\n∂ϕ\\n=∇ωLtask(ω⋆)·\\x12∂2L(ω, θ⋆)\\n∂ω2\\x13−1\\n·∂2L(ω, θ⋆)\\n∂θ∂ω\\x0c\\x0c\\x0c\\x0c\\x0c\\nω⋆(θ⋆)·\\x12∂2Lpred(θ, ϕ)\\n∂θ2\\x13−1\\n·∂2Lpred(θ, ϕ)\\n∂ϕ∂θ\\x0c\\x0c\\x0c\\x0c\\x0c\\nθ⋆(ϕ)\\n(12)\\n4.3.2 Experimental results\\nWe replicated experiments from Nikishin et al. [2022] on the Cartpole environment. The first\\nexperiment involved state distractions, where the state of the agent was augmented with noisy and\\nuninformative values. In this setting, we considered an unconditional diagonal metric of dimension n,\\nwhich is the dimension of the state space, i.e. Λϕ:=diag(ϕ), where ϕ∈Rn. As shown in fig. 6, the\\nMLE method performed the worst across different numbers of distracting states, as it allocated its\\ncapacity to learn distracting states as well. TaskMet outperformed the other methods in all scenarios.\\n9 0 50K 100K 150K 200K\\nEnvironment Steps0100200300400500Episode Return\\n0 50K 100K 150K 200K\\nEnvironment Steps101\\n100101Dynamics (MSE)TaskMet MLE OMD\\nFigure 8: Results on cartpole with a reduced model capacity from Nikishin et al. [2022].\\nThe superior performance of TaskMet with distracting states can be attributed to the metric’s ability\\nto explicitly distinguish informative states from noise states using the task loss and then train the\\nmodel using the given metric, as shown in fig. 7. The learned metric in fig. 7 assigned the highest\\nweight to the third dimension of the state space, which corresponds to the pole angle — the most\\nindicative dimension for the reward. This shows that the metric can differentiate state dimensions\\nbased on their importance to the task.\\nWe also consider a setting with reduced model capacity, where the network is under-parametrized,\\nforcing the model to prioritize how it allocates its capacity. In this scenario, we employ a full\\nconditional metric, denoted as Λϕ= Λ ϕ(x), which enables the metric to weigh dimensions and\\nstate-action pairs differently. We conducted the experiment using a model size of 3 hidden units in\\nthe layer. As depicted in fig. 8, TaskMet achieves a better return on evaluation compared to MLE\\nand OMD. Additionally, it is evident that TaskMet achieves a lower MSE on the model predictions\\ncompared to OMD, indicating that learning with the metric contributes to a better dynamics model.\\n5 Conclusion and discussion\\nIn conclusion, this paper addresses the challenge of combining task and prediction losses in task-\\nbased model learning. While task-based learning methods offer the advantage of discovering task-\\nrelevant features and data samples without explicit inductive biases, the current trend of using\\ntask loss alongside prediction loss has potential limitations. These limitations include overfitting\\nof the prediction model to a specific task, rendering it ineffective for other tasks, and the lack of\\ninterpretability in the task-relevant features learned by the prediction model.\\nTo overcome these limitations, the paper introduces the concept of task-driven metric learning, which\\nintegrates the task loss into a parameterized prediction loss. This approach enables end-to-end\\nlearning of metrics to train prediction models, allowing the models to focus on task-relevant features\\nand dimensions in the prediction space. Moreover, the resulting prediction models become more\\ninterpretable, as metric learning serves as a preconditioning step for gradient-based model training.\\nThe effectiveness of the method is shown using different scales of experimental setting - decision\\noriented tasks as well as downstream learning tasks.\\nOne of the limitations of the method is stability of learning the metric. Bad gradients can lead\\ncollapsed metric which can lead to unrecoverable bad predictions. Hence, hyper-parameter tuning\\nof learning rate for metric learning and parameterization choices of the metric are crucial. Possible\\nextensions to this work includes end-to-end metric learning with multiple task losses, learning metric\\nfor training dynamics models to be used for long-horizon planning tasks, etc.\\nAcknowledgments\\nWe would like to thank Brian Karrer, Claas V oelcker, Karen Ullrich, Leon Bottou, Maximilian Nickel,\\nand Mike Rabbat insightful comments and discussions.\\n10 References\\nAkshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter.\\nDifferentiable convex optimization layers. Advances in neural information processing systems , 32,\\n2019. Cited on page 6.\\nBrandon Amos, Ivan Jimenez, Jacob Sacks, Byron Boots, and J Zico Kolter. Differentiable mpc for\\nend-to-end planning and control. Advances in neural information processing systems , 31, 2018.\\nCited on page 2.\\nAnonymous. Predict-then-optimize via learning to optimize from features. In Submitted to The Twelfth\\nInternational Conference on Learning Representations , 2023. URL https://openreview.net/\\nforum?id=jvOvJ3XSjK . under review. Cited on page 3.\\nSomil Bansal, Roberto Calandra, Ted Xiao, Sergey Levine, and Claire J Tomlin. Goal-driven\\ndynamics learning via bayesian optimization. In 2017 IEEE 56th Annual Conference on Decision\\nand Control (CDC) , pages 5168–5173. IEEE, 2017. Cited on page 8.\\nYoshua Bengio. Using a financial training criterion rather than a prediction criterion. International\\njournal of neural systems , 8(04):433–443, 1997. Cited on pages 2 and 3.\\nMohak Bhardwaj, Byron Boots, and Mustafa Mukadam. Differentiable gaussian process motion\\nplanning. In 2020 IEEE international conference on robotics and automation (ICRA) , pages\\n10598–10604. IEEE, 2020. Cited on page 2.\\nMathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-López,\\nFabian Pedregosa, and Jean-Philippe Vert. Efficient and modular implicit differentiation. Advances\\nin neural information processing systems , 35:5230–5242, 2022. Cited on page 5.\\nTsai-Hsuan Chung, Vahid Rostami, Hamsa Bastani, and Osbert Bastani. Decision-aware learning for\\noptimizing health supply chains. arXiv preprint arXiv:2211.08507 , 2022. Cited on page 2.\\nUlisse Dini. Analisi infinitesimale . Lithografia Gorani, 1878. Cited on page 14.\\nAsen L Dontchev and R Tyrrell Rockafellar. Implicit functions and solution mappings , volume 543.\\nSpringer, 2009. Cited on page 14.\\nPriya Donti, Brandon Amos, and J Zico Kolter. Task-based end-to-end model learning in stochastic\\noptimization. Advances in neural information processing systems , 30, 2017. Cited on pages 2, 3, 4,\\nand 6.\\nOthman El Balghiti, Adam N Elmachtoub, Paul Grigas, and Ambuj Tewari. Generalization bounds\\nin the predict-then-optimize framework. Advances in neural information processing systems , 32,\\n2019. Cited on pages 2 and 3.\\nAdam N Elmachtoub and Paul Grigas. Smart “predict, then optimize”. Management Science , 68(1):\\n9–26, 2022. Cited on pages 2 and 3.\\nAmir-massoud Farahmand. Iterative value-aware model learning. Advances in Neural Information\\nProcessing Systems , 31, 2018. Cited on page 2.\\nAmir-massoud Farahmand, Andre Barreto, and Daniel Nikovski. Value-aware loss function for\\nmodel-based reinforcement learning. In Artificial Intelligence and Statistics , pages 1486–1494.\\nPMLR, 2017. Cited on pages 2, 3, and 8.\\nAaron M Ferber, Taoan Huang, Daochen Zha, Martin Schubert, Benoit Steiner, Bistra Dilkina,\\nand Yuandong Tian. Surco: Learning linear surrogates for combinatorial nonlinear optimization\\nproblems. In International Conference on Machine Learning , pages 10034–10052. PMLR, 2023.\\nCited on page 3.\\nK Ruwani M Fernando and Chris P Tsokos. Dynamically weighted balanced loss: class imbalanced\\nlearning and confidence calibration of deep neural networks. IEEE Transactions on Neural\\nNetworks and Learning Systems , 33(7):2940–2951, 2021. Cited on page 3.\\n11 Joseph Futoma, Michael C Hughes, and Finale Doshi-Velez. Popcorn: Partially observed prediction\\nconstrained reinforcement learning. arXiv preprint arXiv:2001.04032 , 2020. Cited on page 6.\\nRishabh Gupta and Qi Zhang. Data-driven decision-focused surrogate modeling. arXiv preprint\\narXiv:2308.12161 , 2023. Cited on page 3.\\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning\\nbehaviors by latent imagination. arXiv preprint arXiv:1912.01603 , 2019a. Cited on page 3.\\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James\\nDavidson. Learning latent dynamics for planning from pixels. In International conference on\\nmachine learning , pages 2555–2565. PMLR, 2019b. Cited on page 3.\\nNicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive\\ncontrol. arXiv preprint arXiv:2203.04955 , 2022. Cited on page 3.\\nTrevor Hastie and Robert Tibshirani. Discriminant adaptive nearest neighbor classification and\\nregression. Advances in neural information processing systems , 8, 1995. Cited on page 3.\\nSøren Hauberg, Oren Freifeld, and Michael Black. A geometric take on metric learning. Advances in\\nNeural Information Processing Systems , 25, 2012. Cited on page 3.\\nBingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, and Trevor Darrell. Few-shot object\\ndetection via feature reweighting. In Proceedings of the IEEE/CVF International Conference on\\nComputer Vision , pages 8420–8429, 2019. Cited on pages 2 and 3.\\nMahmut Kaya and Hasan ¸ Sakir Bilge. Deep metric learning: A survey. Symmetry , 11(9):1066, 2019.\\nCited on page 3.\\nBrian Kulis et al. Metric learning: A survey. Foundations and Trends ®in Machine Learning , 5(4):\\n287–364, 2013. Cited on page 3.\\nNathan Lambert, Brandon Amos, Omry Yadan, and Roberto Calandra. Objective mismatch in\\nmodel-based reinforcement learning. arXiv preprint arXiv:2002.04523 , 2020. Cited on pages 4\\nand 8.\\nMo Liu, Paul Grigas, Heyuan Liu, and Zuo-Jun Max Shen. Active learning in the predict-then-\\noptimize framework: A margin-based approach. arXiv preprint arXiv:2305.06584 , 2023. Cited on\\npages 2 and 3.\\nJonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by\\nimplicit differentiation. In International Conference on Artificial Intelligence and Statistics , pages\\n1540–1552. PMLR, 2020. Cited on page 5.\\nJayanta Mandi, Peter J Stuckey, Tias Guns, et al. Smart predict-and-optimize for hard combina-\\ntorial optimization problems. In Proceedings of the AAAI Conference on Artificial Intelligence ,\\nvolume 34, pages 1603–1610, 2020. Cited on pages 2 and 3.\\nHarry M Markowitz and G Peter Todd. Mean-variance analysis in portfolio choice and capital\\nmarkets , volume 66. John Wiley & Sons, 2000. Cited on page 7.\\nRichard O Michaud. The markowitz optimization enigma: Is ‘optimized’optimal? Financial analysts\\njournal , 45(1):31–42, 1989. Cited on page 7.\\nEvgenii Nikishin, Romina Abachi, Rishabh Agarwal, and Pierre-Luc Bacon. Control-oriented model-\\nbased reinforcement learning with implicit differentiation. In Proceedings of the AAAI Conference\\non Artificial Intelligence , volume 36, pages 7886–7894, 2022. Cited on pages 2, 3, 6, 8, 9, 10, 14,\\nand 15.\\nTrong Huy Phan and Kazuma Yamamoto. Resolving class imbalance in object detection with\\nweighted cross entropy losses. arXiv preprint arXiv:2006.01413 , 2020. Cited on page 3.\\nAndré Susano Pinto, Alexander Kolesnikov, Yuge Shi, Lucas Beyer, and Xiaohua Zhai. Tuning\\ncomputer vision models with task rewards. arXiv preprint arXiv:2302.08242 , 2023. Cited on\\npage 3.\\n12 Utsav Sadana, Abhilash Chenreddy, Erick Delage, Alexandre Forel, Emma Frejinger, and Thibaut\\nVidal. A survey of contextual optimization methods for decision making under uncertainty, 2023.\\nCited on page 3.\\nSanket Shah, Kai Wang, Bryan Wilder, Andrew Perrault, and Milind Tambe. Decision-focused\\nlearning without decision-making: Learning locally optimized decision losses. In Alice H. Oh,\\nAlekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information\\nProcessing Systems , 2022. URL https://openreview.net/forum?id=eN2lQxjWL05 . Cited\\non pages 2, 3, 6, 7, 8, and 14.\\nSanket Shah, Andrew Perrault, Bryan Wilder, and Milind Tambe. Leaving the nest: Going beyond\\nlocal loss functions for predict-then-optimize. arXiv preprint arXiv:2305.16830 , 2023. Cited on\\npage 3.\\nGuangyao Shi and Pratap Tokekar. Decision-oriented learning with differentiable submodular\\nmaximization for vehicle routing problem. arXiv preprint arXiv:2303.01543 , 2023. Cited on\\npage 2.\\nClaas V oelcker, Victor Liao, Animesh Garg, and Amir-massoud Farahmand. Value gradient weighted\\nmodel-based reinforcement learning. arXiv preprint arXiv:2204.01464 , 2022. Cited on pages 2, 3,\\n4, and 8.\\nRushil V ohra, Ali Rajaei, and Jochen L Cremer. End-to-end learning with multiple modalities for\\nsystem-optimised renewables nowcasting. arXiv preprint arXiv:2304.07151 , 2023. Cited on\\npage 2.\\nKai Wang, Shresth Verma, Aditya Mate, Sanket Shah, Aparna Taneja, Neha Madhiwalla, Aparna\\nHegde, and Milind Tambe. Decision-focused learning in restless multi-armed bandits with ap-\\nplication to maternal and child care domain. arXiv preprint arXiv:2202.00916 , 2022. Cited on\\npage 2.\\nKilian Q Weinberger and Gerald Tesauro. Metric learning for kernel regression. In Artificial\\nintelligence and statistics , pages 612–619. PMLR, 2007. Cited on page 3.\\nBryan Wilder, Bistra Dilkina, and Milind Tambe. Melding the data-decisions pipeline: Decision-\\nfocused learning for combinatorial optimization. In Proceedings of the AAAI Conference on\\nArtificial Intelligence , volume 33, pages 1658–1665, 2019. Cited on pages 2, 3, 6, and 7.\\nRuihan Wu, Chuan Guo, Awni Hannun, and Laurens van der Maaten. Fixes that fail: Self-defeating\\nimprovements in machine-learning systems. Advances in Neural Information Processing Systems ,\\n34:11745–11756, 2021. Cited on page 3.\\nYujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, and Tomas\\nPfister. Differentiable top-k with optimal transport. Advances in Neural Information Processing\\nSystems , 33:20520–20531, 2020. Cited on page 6.\\nLiu Yang and Rong Jin. Distance metric learning: A comprehensive survey. Michigan State Universiy ,\\n2(2):4, 2006. Cited on page 3.\\nArman Zharmagambetov, Brandon Amos, Aaron Ferber, Taoan Huang, Bistra Dilkina, and Yuandong\\nTian. Landscape surrogate: Learning decision losses for mathematical optimization under partial\\ninformation. arXiv preprint arXiv:2307.08964 , 2023. Cited on page 3.\\n13 A The implicit function theorem\\nWe used the implicit function theorem to compute the derivative of the prediction model with respect\\nto the metric’s parameters in eq. (7). For completeness, this section briefly presents the standard\\nimplicit function theorem, cf.Dini [1878] and Dontchev and Rockafellar [2009, Theorem 1B.1]:\\nTheorem 1 (Implicit Function Theorem) Implicit Function Theorem : Letf:Rn×Rm→Rbe a\\ncontinuous differentiable function, and let x⋆, y⋆be a point satisfying f(x⋆, y⋆) = 0 . If the Jacobian\\n∂(f(x⋆,y⋆)\\n∂y̸= 0, then there exists an open set around (x⋆, y⋆)and a unique continuously differentiable\\nfunction gsuch that y⋆=g(x⋆)andf(x, g(x)) = 0 . Additionally, the following relation holds:\\n∂g(x)\\n∂x=−\\x12∂f(x, y⋆)\\n∂y\\x13∂f(x, y⋆)\\n∂x|y⋆=g(x) (13)\\nB Implementation Details\\nB.1 Decision Oriented Model Learning\\nWe replicated our experiments using the codebase provided by Shah et al. [2022], which can be\\nfound on github. To ensure consistency, we used the same hyperparameters as mentioned in the\\ncode or article for the baselines. Our metric learning pipeline was added on top of their code, and\\nthus we focused on tuning hyperparameters related to metric learning. The metric is parameterized\\nasΛϕ(x) =Lϕ(x)L⊤\\nϕ(x) +ϵϕIn×n, where ϵϕis a learnable parameter that explicitly controls the\\namount of Euclidean metric in the predicted metric. This helps ensure the stability of metric learning.\\nWe initialize the parameters in such a way that the predicted metric is close to the Euclidean metric.\\nFor each outer loop of metric update, we perform Kinner updates to train the predictor. Following\\nthe methodology of Shah et al. [2022], we conducted 50 runs with different seeds for each of the\\nexperiments, where each method was evaluated on 10 different datasets, with 5 different seeds used\\nfor each dataset.\\nTable 4: Hyper-parameters for Decision Oriented Learning Experiments\\nHyper-Parameter Values\\nΛϕlearning rate 10−3\\nΛϕhidden layer sizes [200]\\nWarmup steps 500\\nInner Iterations ( K)100\\nImplicit derivative batchsize 10\\nImplicit derivative solver Conjugate gradient on the normal equations ( 5iterations)\\nB.2 Model-Based Reinforcement Learning\\nWe consider the work of Nikishin et al. [2022] as the baseline for replicating the experiments, and\\nwe build upon their source code. Our metric learning is just one additional step to their method. We\\nadopt exact same hyperparameters as their for dynamics learning and Action-Value function learning.\\nWe focus on exploring and tuning the hyper-parameters specific to the metric learning component of\\nthe method.\\nTable 5: Hyper-parameters for the CartPole experiments\\nHyper-Parameter Values\\nΛϕlearning rate 10−3\\nΛϕhidden layer sizes [32, 32]\\nWarmup steps 5000\\nInner iterations ( K)1\\nImplicit derivative batchsize 256\\nImplicit derivative solver Conjugate gradient on the normal equations (10 iterations)\\n14 For the state distractor experiments, we parameterize the metric as an unconditional diagonal matrix,\\ndenoted as Λϕ=diag(ϕ)where ϕ∈Rnandnis the dimension of the state space. In addition,\\nwe consider a hyper-parameter of metric parameterization , for which we either take normalize or\\nunnormalized metric. When we refer to normalizing the metric, we mean constraining the norm\\nof the ϕvector to be equal to the L2 norm of an euclidean metric which is used by MSE method.\\nThis constrains the family of learnable metrics. To achieve this, we set ϕ:=ϕ\\n∥ϕ∥2√n, ensuring\\n∥ϕ∥2=∥In×n∥2=√n. We also used L1 regularization on the metric output, to induce sparsity in\\nthe metric. We sweep over three values of the regularization coefficient - [0.0,0.001,0.1]. We ran a\\nsweep over the 6combinations of hyperparameters - [unnormalized, normalized ]×[0.0,0.001,0.1]\\nand choose the best hyper-parameter combination for each of the experiment. All the number reported\\nin the experiments are calculated over 10random seeds.\\nOur metric learning approach uses two implicit gradient steps. Firstly, we take the implicit derivative\\nthrough action-value network parameters, approximating the inverse hessian to the identity, similar to\\nNikishin et al. [2022]. Secondly, for the step through dynamics network parameters, we calculate the\\nexact implicit derivative.\\n15'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ee70560a-ae43-44cf-89fb-1f3ac9d3e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "16390c11-df1f-415f-bff2-a4ffc67dee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='TaskMet: Task-Driven Metric Learning for Model Learning\\nDishank Bansal∗Ricky T. Q. Chen Mustafa Mukadam Brandon Amos\\nMeta\\nAbstract\\nDeep learning models are often deployed in downstream tasks that the training\\nprocedure may not be aware of. For example, models solely trained to achieve\\naccurate predictions may struggle to perform well on downstream tasks because\\nseemingly small prediction errors may incur drastic task errors. The standard end-' metadata={'start_index': 0}\n",
      "page_content='to-end learning approach is to make the task loss differentiable or to introduce a\\ndifferentiable surrogate that the model can be trained on. In these settings, the task\\nloss needs to be carefully balanced with the prediction loss because they may have\\nconflicting objectives. We propose take the task loss signal one level deeper than the\\nparameters of the model and use it to learn the parameters of the loss function the' metadata={'start_index': 447}\n",
      "page_content='model is trained on, which can be done by learning a metric in the prediction space.\\nThis approach does not alter the optimal prediction model itself, but rather changes\\nthe model learning to emphasize the information important for the downstream task.\\nThis enables us to achieve the best of both worlds: a prediction model trained in the\\noriginal prediction space while also being valuable for the desired downstream task.' metadata={'start_index': 871}\n",
      "page_content='We validate our approach through experiments conducted in two main settings:\\n1) decision-focused model learning scenarios involving portfolio optimization\\nand budget allocation, and 2) reinforcement learning in noisy environments with\\ndistracting states. The source code to reproduce our experiments is available here.\\n1 Introduction\\ntrue modelMSETaskMetDFLmodel space\\ntask loss\\nFigure 1: The MSE results in a model\\nclose to the true model in the predic-\\ntion space, but may give poor task per-' metadata={'start_index': 1295}\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([long_text])\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "print(texts[2])\n",
    "print(texts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ab236b0c-1e4e-4477-b86b-d5d1ca169f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TaskMet: Task-Driven Metric Learning for Model Learning\\nDishank Bansal∗Ricky T. Q. Chen Mustafa Mukadam Brandon Amos\\nMeta\\nAbstract\\nDeep learning models are often deployed in downstream tasks that the training\\nprocedure may not be aware of. For example, models solely trained to achieve\\naccurate predictions may struggle to perform well on downstream tasks because\\nseemingly small prediction errors may incur drastic task errors. The standard end-\\nto-end learning approach is to make the task loss differentiable or to introduce a\\ndifferentiable surrogate that the model can be trained on. In these settings, the task\\nloss needs to be carefully balanced with the prediction loss because they may have\\nconflicting objectives. We propose take the task loss signal one level deeper than the\\nparameters of the model and use it to learn the parameters of the loss function the\\nmodel is trained on, which can be done by learning a metric in the prediction space.\\nThis approach does not alter the optimal prediction model itself, but rather changes\\nthe model learning to emphasize the information important for the downstream task.\\nThis enables us to achieve the best of both worlds: a prediction model trained in the\\noriginal prediction space while also being valuable for the desired downstream task.\\nWe validate our approach through experiments conducted in two main settings:\\n1) decision-focused model learning scenarios involving portfolio optimization\\nand budget allocation, and 2) reinforcement learning in noisy environments with\\ndistracting states. The source code to reproduce our experiments is available here.\\n1 Introduction\\ntrue modelMSETaskMetDFLmodel space\\ntask loss\\nFigure 1: The MSE results in a model\\nclose to the true model in the predic-\\ntion space, but may give poor task per-\\nformance. Decision-focused learning\\n(DFL) methods optimize the task loss,\\nbut may deviate from the prediction\\nspace. TaskMet optimizes the task loss\\nwhile retaining the prediction task.Machine learning models for prediction are typically trained\\nto maximize the likelihood on a training dataset. While the\\nmodels are capable of universally approximating the under-\\nlying data generating process to predict the output, they are\\nprone to approximation errors due to limited training data and\\nmodel capacity. These errors lead to suboptimal performance\\nin downstream tasks where the models are used. Furthermore,\\neven though a model may appear to have reasonable predictive\\nperformance on the metric and training data it was trained on,\\nsuch as the mean squared error, employing the model for a\\ndownstream task may require the model to focus on different\\nparts of the data that were not emphasized in the training for\\npredictive performance. Overcoming the discrepancy between\\nthe model’s prediction task and performance on a downstream\\ntask is the focus of our paper.\\n∗Work done as part of the Meta AI residency program.\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2312.05250v1  [cs.LG]  8 Dec 2023 Table 1: Settings we focus on where there is a discrepancy between the prediction task of a model\\nand the downstream task where the model is deployed, i.e., Lpred̸=Ltask.\\n(x) ( y) ( Lpred) ( Ltask)\\nSetting Features Predictions Prediction Loss Task Loss\\nPortfolio optimization Stock information Expected return of a stock MSE Portfolio’s performance\\nBudget allocation Item information Value of item MSE Allocation’s performance\\nModel-based RL Current state and action Next state MSE Value estimation given the model\\nExamples of settings where the model’s prediction loss Lpredis mis-matched from the downstream\\ntaskLtaskinclude the following, which table 1 also summarizes:\\n1.theportfolio optimization setting from Wilder et al. [2019], which predicts the expected\\nreturns from stocks for a financial portfolio. Here, the Lpredis the MSE and Ltaskis from the\\nregret of running a portfolio optimization problem on the output;\\n2.theallocation setting from Wilder et al. [2019], which predicts the value of items that are\\nbeing allocated, e.g. click-through-rates for recommender systems. Here, Lpredis the MSE\\nandLtaskmeasures the result of allocating the highest-value items.\\n3.themodel-based reinforcement learning setting of learning the system dynamics from\\nNikishin et al. [2022]. Here, Lpredis the MSE of dynamics model and the Ltaskmeasures\\nhow well the agent performs for downstream value predictions.\\nMotivated by examples such as in table 1, the research topics of end-to-end task-based model learning\\n[Bengio, 1997, Donti et al., 2017], decision-focused learning [Wilder et al., 2019], and Smart “Predict,\\nthen Optimize” [Elmachtoub and Grigas, 2022] study how to use information from the downstream\\ntask to improve the model’s performance on that particular task. Task-based learning has applications\\nin financial price predictions [Bengio, 1997, Elmachtoub and Grigas, 2022], inventory stock, demand,\\nand price forecasting [Donti et al., 2017, Elmachtoub and Grigas, 2022, El Balghiti et al., 2019,\\nMandi et al., 2020, Liu et al., 2023], dynamics modeling for model-based reinforcement learning\\n[Farahmand et al., 2017, Amos et al., 2018, Farahmand, 2018, Bhardwaj et al., 2020, V oelcker et al.,\\n2022, Nikishin et al., 2022], renewable nowcasting [V ohra et al., 2023], vehicular routing [Shi and\\nTokekar, 2023], restless multi-armed bandits for maternal and child care [Wang et al., 2022], medical\\nresource allocation [Chung et al., 2022], and budget allocation, matching, and recommendation\\nproblems [Kang et al., 2019, Wilder et al., 2019, Shah et al., 2022].\\nLimitations of task-based learning. Task-based model learning comes with the goal of being able\\nto discover task-relevant features and data-samples on its own without the need of explicit inductive\\nbiases. The current trend for end-to-end model learning uses task loss along with the prediction loss\\nto train the prediction models. Though easy to use, these methods may be limited by 1) the prediction\\noverfitting to the particular task, rendering it unable to generalize; 2) the need to tuning the weight\\ncombining the task and prediction losses as in eq. (1).\\nOur contributions. We propose one way of overcoming these limitations: use the task-based\\nlearning signal not to directly optimize the weights of the model, but to shape a prediction loss\\nthat is constructed in a way so that the model will always stay in the original prediction space. We\\ndo this in section 3 via metric learning in the prediction space and use the task signal to learn a\\nparameterized Mahalanobis loss. This enables more interpretable learning of the model using the\\nmetric compared to learning with a combination of task loss and prediction loss. The learned metric\\ncan uncover underlying properties of the task that are useful for training the model, e.g. as in figs. 4\\nand 7. Section 4 shows the empirical success of metric learning on decision focused model learning and 7. Section 4 shows the empirical success of metric learning on decision focused model learning\\nand model-based reinforcement learning. Figure 1 illustrates the differences to prior methods.\\n2 Background and related work\\nTask-based model learning . We will mostly focus on solving regression problems where the dataset\\nD:={(xi, yi)}N\\ni=1consists of Ninput-output pairs, which we will assume to be in Euclidean space.\\nThe model makes a prediction ˆy:=fθ(x)and is parameterized by θ. The model has an associated'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6c8b8cf9-19e7-48b7-878e-0bf6116cf5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def is_valid_json(my_json):\n",
    "    try:\n",
    "        json.loads(my_json)\n",
    "        return True\n",
    "    except ValueError as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "class SubjectPresent:\n",
    "    def __init__(self, text_spliter : RecursiveCharacterTextSplitter, model, parser):\n",
    "        self.text_spliter = text_spliter\n",
    "        self.model = model\n",
    "        self.parser = parser\n",
    "\n",
    "    def is_present(self, full_text:str, concept:str):\n",
    "        texts = self.text_spliter.create_documents([full_text])\n",
    "        for text in texts:\n",
    "            print('a')\n",
    "            is_present = self.model.invoke({\"concept\":concept, \"text\":text.page_content})\n",
    "            if is_valid_json(is_present.content):\n",
    "                if bool(self.parser.invoke(is_present)):\n",
    "                    return True\n",
    "            is_present = self.model.invoke({\"concept\":concept, \"text\":is_present.content})\n",
    "            if is_valid_json(is_present.content):\n",
    "                if bool(self.parser.invoke(is_present)):\n",
    "                    return True\n",
    "                    \n",
    "        return False\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "5edfba32-f811-4944-9904-6e801394e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_spliter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fed7a89b-f318-4e7e-b0fd-84bf46f043f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_chain = neuro_concept_present_prompt | llm \n",
    "\n",
    "is_present  = SubjectPresent(text_spliter = basic_spliter, model = simple_chain, parser = binary_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "626a1351-2a91-48c1-b7de-8ce3549b3c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_present.is_present(full_text = long_text, concept = 'despiking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cc9b1611-41f8-4406-be5f-47a7bac46ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'You are a helpful assistant in the field of neuroscience that search if the concept of despiking is present or not in the text.\\n The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"value\": {\"title\": \"Value\", \"description\": \"yes or no value\", \"examples\": [\"yes\", \"no\"], \"type\": \"string\"}}, \"required\": [\"value\"]}\\n```. \\n The text is :  \\n TaskMet: Task-Driven Metric Learning for Model Learning\\nDishank Bansal∗Ricky T. Q. Chen Mustafa Mukadam Brandon Amos\\nMeta\\nAbstract\\nDeep learning models are often deployed in downstream tasks that the training\\nprocedure may not be aware of. For example, models solely trained to achieve\\naccurate predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "97db76d4-42c3-4c5e-804e-4048eede0774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant in the field of neuroscience that search if the concept of despiking is present or not in the text.\\n The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"value\": {\"title\": \"Value\", \"description\": \"yes or no value\", \"examples\": [\"yes\", \"no\"], \"type\": \"string\"}}, \"required\": [\"value\"]}\\n```. \\n The text is :  \\n TaskMet: Task-Driven Metric Learning for Model Learning\\nDishank Bansal∗Ricky T. Q. Chen Mustafa Mukadam Brandon Amos\\nMeta\\nAbstract\\nDeep learning models are often deployed in downstream tasks that the training\\nprocedure may not be aware of. For example, models solely trained to achieve\\naccurate predict'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1a3fdc0e-3124-4295-a5a7-b04d56170a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The concept of despiking is not present in the given text.')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_chain.invoke({\"concept\":\"despiking\", \"text\":texts[0].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6eacd178-9dda-43d5-bb3d-2173429eda8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TaskMet: Task-Driven Metric Learning for Model Learning\\nDishank Bansal∗Ricky T. Q. Chen Mustafa Mukadam Brandon Amos\\nMeta\\nAbstract\\nDeep learning models are often deployed in downstream tasks that the training\\nprocedure may not be aware of. For example, models solely trained to achieve\\naccurate predictions may struggle to perform well on downstream tasks because\\nseemingly small prediction errors may incur drastic task errors. The standard end-\\nto-end learning approach is to make the task loss differentiable or to introduce a\\ndifferentiable surrogate that the model can be trained on. In these settings, the task\\nloss needs to be carefully balanced with the prediction loss because they may have\\nconflicting objectives. We propose take the task loss signal one level deeper than the\\nparameters of the model and use it to learn the parameters of the loss function the\\nmodel is trained on, which can be done by learning a metric in the prediction space.\\nThis approach does not alter the optimal prediction model itself, but rather changes\\nthe model learning to emphasize the information important for the downstream task.\\nThis enables us to achieve the best of both worlds: a prediction model trained in the\\noriginal prediction space while also being valuable for the desired downstream task.\\nWe validate our approach through experiments conducted in two main settings:\\n1) decision-focused model learning scenarios involving portfolio optimization\\nand budget allocation, and 2) reinforcement learning in noisy environments with\\ndistracting states. The source code to reproduce our experiments is available here.\\n1 Introduction\\ntrue modelMSETaskMetDFLmodel space\\ntask loss\\nFigure 1: The MSE results in a model\\nclose to the true model in the predic-\\ntion space, but may give poor task per-\\nformance. Decision-focused learning\\n(DFL) methods optimize the task loss,\\nbut may deviate from the prediction\\nspace. TaskMet optimizes the task loss\\nwhile retaining the prediction task.Machine learning models for prediction are typically trained\\nto maximize the likelihood on a training dataset. While the\\nmodels are capable of universally approximating the under-\\nlying data generating process to predict the output, they are\\nprone to approximation errors due to limited training data and\\nmodel capacity. These errors lead to suboptimal performance\\nin downstream tasks where the models are used. Furthermore,\\neven though a model may appear to have reasonable predictive'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "419e8e20-f9a0-418e-af11-0ec3439ef72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_chain = neuro_concept_present_prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1f91b8f7-c300-4af4-bd4c-ba8265d0e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = simple_chain.invoke({\"concept\":\"despiking\", \"text\":texts[0].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "20f1e452-9fda-44c6-a0d3-5a21a7cb3470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"value\": \"no\"}'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0ef2506e-6ad5-4fe0-a00c-84aaae6fa3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Binary(value='no')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_parser.invoke(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ac77ccff-5782-4aa3-a11f-e519fcc2208b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Binary(value='no')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefed2b2-a3aa-4666-a618-b5876797af9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
